{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c9f0c5ec5fb2e6026a1966c620efe33",
     "grade": false,
     "grade_id": "cell-934b180c93302dfa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c73ee687de3cea37e5f9b23fc68dda9",
     "grade": false,
     "grade_id": "cell-1af857b6b6ac9033",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 1 \n",
    "## Question 1: Classifying Digits\n",
    "For this question we are going to be tackling a simple optical character recognition problem: identifying images of hand-written digits using a dataset called MNIST, which is immensely popular for use in introductory ML courses. I have provided you the model to use below (and at this point in the course you can treat it as a “black box”).  The objective of this problem is to identify the optimal value of a particular hyperparameter for the model by using the validation strategies we have learned in class.  \n",
    "\n",
    "After you have added your code, restart your kernel and run the notebook from the top to ensure all cells run properly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8549b029ffe8fdfa429c04512220c11",
     "grade": false,
     "grade_id": "cell-6f18e5af3940d19e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "263c2060491c612fc0879ab4b66370cd",
     "grade": false,
     "grade_id": "cell-56dcb628dc113d10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset we will use\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "640301d3b0196f790e396f7c0e591b0d",
     "grade": false,
     "grade_id": "cell-9b1d2c211850fef8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As we can see above, we have 1797 observations of hand-written digits, each with a shape of 8x8 pixels.  Let's now visualize some of them so we can understand better what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f49820c93855bdbc2ee90aeca80b2d69",
     "grade": false,
     "grade_id": "cell-cf5f5b8da9ff8e34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "# subplot_kw = arguments to be passed to each of the axes in the subplots directly\n",
    "# gridspec_kw = when you need more control over spacing and arrangement of subplots\n",
    "\n",
    "# axes.flat is a convenient way to get all axes at the same time\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
    "            transform=ax.transAxes, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a11c7b1a3f46ae45e28da62b3f58068",
     "grade": false,
     "grade_id": "cell-b0e5d0e1b018865d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Above we can see 25 of the digit images, with the digit label shown in small green font at lower left.  In our modeling project, we will use the values of each pixel (64 in total per image: 8x8) as the features of our input to the model.  Our target will be the image labels indicating what digit the image represents.\n",
    "\n",
    "As we know, to use the Scikit-Learn models our input data is expected to be in the shape [n_observations, n_features].  To get our input data into this form, we need to flatten our image pixel matrix from 8x8 to 1x64 (each row has 64 features - one feature representing each pixel).  Fortunately, someone has already done this for us and we can read the data in using the proper shape as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb75eccc21dc62d57be171301b618ca8",
     "grade": false,
     "grade_id": "cell-ce465a3f07e317ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "# Read in the input and target data\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "# X shape = (N, 64)\n",
    "# Y shape = (N,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a07c4da93d13826ebb7343b0858271f6",
     "grade": false,
     "grade_id": "cell-4490767b900b945a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1.1: Train and test a KNN classification model (8 pts)\n",
    "In this part, we will practice training and evaluating performance of models using a simple K-Nearest-Neighbors (KNN) classifier model.  You do not need to worry about how this model works at this point, but if you are curious, for each datapoint that is fed to it to generate a prediction, it identifies the \"*n_neighbors*\" (a hyperparameter you can adjust) closest datapoints to it found in the training dataset (using either Euclidian distance or another similarity metric) and returns the target label most common to the *n_neighbors* neighbor points.  For example, if you feed an image to the KNN classifier and it identifies that most of the nearby points to the image are labeled as \"3\", it will predict that the image you have just provided is also a \"3\".\n",
    "\n",
    "For this first part, I have provided the code to set up the model below.  What I would like you to do is to complete the function `model_digits()` which accepts as input the input data `X` and the target labels `y` defined above, a model `model`, a value for `random_state` and a value for percentage of data to use in the test set `test_size`.\n",
    "\n",
    "The function should do the following:  \n",
    "- Split the data (X and y) into a training and set set using 80% for the training set and 20% for the test set.  Be sure to set `random_state=0` when splitting the data and stratify based on the target labels\n",
    "- Fit your model using the training data  \n",
    "- Get the predictions on the test set and calculate accuracy of the test set predictions\n",
    "\n",
    "Your function should return the accuracy of the test set predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7e78eebf5a8f0558f7de550f9be979a",
     "grade": false,
     "grade_id": "cell-e060218eb5bb1bd9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def model_digits(X,y,model,random_state=0,test_size=0.2):\n",
    "    '''\n",
    "    Splits the data, trains a model and evaluates the model predictions on the test set using accuracy\n",
    "\n",
    "    Inputs:\n",
    "        X(np.ndarray): training data inputs\n",
    "        y(np.ndarray): training data targets\n",
    "        model(sklearn.base.BaseEstimator): instantiated scikit-learn model object\n",
    "        random_state(int): random seed to use for splitting data\n",
    "        test_size(float): proportion of data to use for the test set\n",
    "\n",
    "    Returns:\n",
    "        acc_test(float): accuracy of the test set predictions\n",
    "    '''\n",
    "    # Still unsure what stratify = y means\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = test_size, random_state = random_state, stratify = y)\n",
    "\n",
    "    # Y_train is of size (m, )\n",
    "    model.fit(X_train, Y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    accuracy = (y_predicted == Y_test).sum() / (y_predicted.size)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additionally defining own implementation of KNN\n",
    "\n",
    "class KNN:\n",
    "    \n",
    "    def __init__(self, n_neighbors):\n",
    "        self.n_neighbors = n_neighbors\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, test):\n",
    "        y_pred = []\n",
    "        for x in test:\n",
    "            X_subtracted = self.X - x\n",
    "            norms = np.linalg.norm(X_subtracted, axis = 1)\n",
    "            idxs_sort_by_distances = norms.argsort()\n",
    "            top_k_idxs = idxs_sort_by_distances[:self.n_neighbors]\n",
    "            nearest_neighbour_labels = self.y[top_k_idxs]\n",
    "            pred_value = np.bincount(nearest_neighbour_labels).argmax()\n",
    "            y_pred.append(pred_value)\n",
    "\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975\n"
     ]
    }
   ],
   "source": [
    "knn = KNN(5)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "knn.fit(X_train, Y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "acc = (y_pred == Y_test).sum() / (Y_test.size)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54af9483359ef7d77483dce01c235a4e",
     "grade": true,
     "grade_id": "cell-ef483e1a9a02bb23",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of your classifier on the test set is 0.978\n",
      "Passed test\n"
     ]
    }
   ],
   "source": [
    "# Test cell\n",
    "\n",
    "# Set up the model we will use\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Apply your function\n",
    "acc_test = model_digits(X,y,knn_model,random_state=0,test_size=0.2)\n",
    "print('Accuracy of your classifier on the test set is {:.3f}'.format(acc_test))\n",
    "\n",
    "assert np.round(acc_test,2)==0.98\n",
    "print('Passed test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8577e95d459389212bfe213fd295ce8a",
     "grade": false,
     "grade_id": "cell-4110d3220df7cfc5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1.2A: Perform algorithm selection using a validation set (12 pts)\n",
    "In this part, we will compare performance of two different classifer algorithms: a KNN model as above and a multilayer perceptron (neural network) model.  We will not do any hyperparameter tuning for either model, but rather will focus on comparing the two algorithms as they are provided in the cell below.  Your task is to compare models using a fixed validation set (NOT cross-validation, yet).  \n",
    "\n",
    "To do this, complete the below function `compare_models()` which takes as input the data `X` and labels `y` defined above, a list of models to compare, the percentage of data to use for the test set `test_size`, and the percentage of the training set to use for validation `val_size`.\n",
    "\n",
    "Your function should do the following:  \n",
    "- Split your data into a test set and training set using the input `test_size`, being sure to use `random_state=0` while splitting and stratify based on the target labels \n",
    "- Further split your training set into a validation set and training subset using `val_size`, being sure to use `random_state=0` while splitting and stratify based on the target labels \n",
    "- Fit each model on the training set  \n",
    "- Get the predictions of each model on the validation set and calculate the validation accuracy. \n",
    "- Select the model with the higher validation accuracy as your final model. \n",
    "- Retrain your final selected model on the combined training+validation set\n",
    "- Using your retrained final model, calculate the predictions on the test set and the accuracy of the test set predictions. \n",
    "\n",
    "Your function should return your selected best model and the test set accuracy performance of your selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d57ba0743a4208c9a35e93078340e6cd",
     "grade": false,
     "grade_id": "cell-d4c6a7c652fdd51a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compare_models(X,y,models,test_size,val_size,random_state=0):\n",
    "    '''\n",
    "    Splits data to create validation and test sets, compares models based on validation set performance and then calculates the \n",
    "    test set accuracy of the model with the highest validation set performance\n",
    "\n",
    "    Inputs:\n",
    "        X(np.ndarray): input data\n",
    "        y(np.ndarray): labels\n",
    "        models(list): list of instantiated scikit-learn model objects\n",
    "        test_size(float): proportion of the data to use for the test set\n",
    "        val_size(float): proportion of the training set data to use as a validation set\n",
    "        random_state(int): random seed to use for splitting data\n",
    "\n",
    "    Returns:\n",
    "        best_model(sklearn.base.BaseEstimator): model with the highest validation set performance\n",
    "        acc_test(float): accuracy of the best model on the test set\n",
    "    '''\n",
    "    X_Train, X_test, y_Train, y_test = train_test_split(X, y, test_size=test_size,\n",
    "                                                        random_state = random_state, stratify = y)\n",
    "    X_train, X_val, y_train, y_val =  train_test_split(X_Train, y_Train, test_size = val_size, random_state=random_state, stratify=y_Train)\n",
    "\n",
    "    assert(X_train.shape[0] + X_val.shape[0] + X_test.shape[0] == X.shape[0])\n",
    "    assert(y_train.shape[0] + y_val.shape[0] + y_test.shape[0] == y.shape[0])\n",
    "\n",
    "    acc_list = []\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        acc = (y_pred == y_val).sum() / y_pred.size\n",
    "        acc_list.append(acc)\n",
    "    \n",
    "    # Picking out the best model\n",
    "    idx_best_model = np.argmax(acc_list)\n",
    "    best_model = models[idx_best_model]\n",
    "    \n",
    "    # Training the best model now on the complete training data\n",
    "    best_model.fit(X_Train, y_Train)\n",
    "\n",
    "    y_Pred = best_model.predict(X_test)\n",
    "    acc = (y_Pred == y_test).sum() / y_Pred.size\n",
    "    return best_model, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d06d698e91a20f7183032c8fb6a3aa50",
     "grade": true,
     "grade_id": "cell-ce476d23f85f565d",
     "locked": true,
     "points": 12,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of your final model on the test set is 0.978\n",
      "Passed tests\n"
     ]
    }
   ],
   "source": [
    "# Test cell\n",
    "\n",
    "# Set up the two models to compare\n",
    "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "model_mlp = MLPClassifier(hidden_layer_sizes=(100,50),activation='relu',\n",
    "                      solver='sgd',learning_rate_init=0.01,max_iter=2000)\n",
    "models = [model_knn,model_mlp]\n",
    "\n",
    "best_model, testacc = compare_models(X,y,models,test_size=0.2,val_size=0.3,random_state=0)\n",
    "print('Accuracy of your final model on the test set is {:.3f}'.format(testacc))\n",
    "\n",
    "assert best_model == model_knn\n",
    "assert np.round(testacc,2) == 0.98\n",
    "print('Passed tests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.2B (10 pts)\n",
    "In Q1.2A, we compared two different algorithms by using their accuracy score on the validation set to select the better performing algorithms.  When working on a real-life industry project, list at least two other considerations when selecting an algorithm to use.  Then, for each of the two considerations you list, provide an example of a situation when your algorithm selection decision would be heavily influenced by this criterion and explain why it would heavily influence your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "1)\tInterpretability of the model  \n",
    "2)\tComputational resources required to train and run model\n",
    "\n",
    "5pts for each.  For full credit must also provide a reasonable example of when each criterion would be important and explain why.  -3 points for each of no example is provided.\n",
    "\n",
    "Student can get credit for other criteria besides the 2 above if the criteria they list are reasonable considerations when selecting an algorithm.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb8445b5d1828977e79120c9ed9cfddb",
     "grade": false,
     "grade_id": "cell-943ddb6cd56a80da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1.3: Tune hyperparameters using cross-validation (15 pts)\n",
    "For this part, we are again going to use the KNN algorithm which we used above.  However, rather than leaving its hyperparameter *n_neighbors* at a fixed value, we are going to run it with different values and determine which value gives us the best performance.  We will then use that in our final model.  To compare versions of the model using different values of *n_neighbors* we will use cross-validation.\n",
    "\n",
    "Your objective is to find the optimal value of `n_neighbors` using cross-validation and then to evaluate the performance of your best model.  To do this, complete the below function `crossval_compare()` which takes as input the data `X` and labels `y`, a list of values `neighbors_list` to try for the hyperparameter `n_neighbors` of the model, the number of iterations `k` to use in the cross-validation, the amount of data to reserve for the test set `test_size`, and `random_state=0`.  \n",
    "\n",
    "Your function should perform the following:  \n",
    "- Split the data into training and test sets.  Be sure to set `random_state=0` and stratify using the target labels\n",
    "- For each integer value `n` in the list `neighbors_list`:\n",
    "    - Instantiate a KNeighborsClassifier model setting `n_neighbors = n`  \n",
    "    - Perform a K-folds cross-validation, training the model on the training folds then evaluating the accuracy of the model on the validation fold.  Set `shuffle=False` (done by default) when performing the K-fold cross validation\n",
    "    - Calculate and store the mean validation accuracy\n",
    "- Identify the value of `n_neighbors` which yielded the highest mean validation accuracy  \n",
    "- Create a final KNeighborsClassifier model using the optimal `n_neighbors` value from above. \n",
    "- Fit your final model and then use it to get the predictions for the test set. \n",
    "- Calculate the accuracy of your final model on the test set\n",
    "\n",
    "Your function should return a tuple of the optimal value of `n_neighbors` from the range as an integer, and the accuracy of your final optimal model on the test set as a float.\n",
    "\n",
    "Note: you may NOT use scikit-learn's `cross_val_score()` method for this.  You may only use what is imported for you at top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf6f0b376d4a0e2692274b73062b141b",
     "grade": false,
     "grade_id": "cell-42937c2a73ef506e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def crossval_compare(X,y,n_list,k,test_size,random_state=0):\n",
    "    '''\n",
    "    Use cross-validation to determine the optimal value of the hyperparameter n_neighbors from the input list and then\n",
    "    calculate the accuracy of the model using the optimal n_neighbors on the test set\n",
    "\n",
    "    Inputs:\n",
    "        X(np.ndarray): input data\n",
    "        y(np.ndarray): labels\n",
    "        n_list(list): list of possible values for n_neighbors hyperparameter\n",
    "        k(int): number of folds to use for k-folds cross validation\n",
    "        test_size(float): proportion of the data to use for the test set\n",
    "        random_state(int): random seed to use for splitting data\n",
    "\n",
    "    Returns:\n",
    "        opt_nval(int): the optimal n_neighbors value from the input list which results in the highest cross-validated performance\n",
    "        acc_test(float): the accuracy of the best model (using opt_nval) on the test set\n",
    "    '''\n",
    "    kf = KFold(n_splits=k)\n",
    "    X_Train, X_test, y_Train, y_test = train_test_split(X, y, test_size = test_size, random_state=random_state, stratify=y)\n",
    "    mean_accs = []\n",
    "    for n_neighbours in n_list:\n",
    "        model = KNeighborsClassifier(n_neighbors=n_neighbours)\n",
    "        acc_vals = []\n",
    "        for train_idx, val_idx in kf.split(X_Train):\n",
    "            X_train, y_train = X_Train[train_idx], y_Train[train_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            X_val, y_val = X_Train[val_idx], y_Train[val_idx]\n",
    "            y_pred = model.predict(X_val)\n",
    "            acc = (y_pred == y_val).sum() / (y_pred.size)\n",
    "            acc_vals.append(acc)\n",
    "        mean_accs.append(np.mean(acc_vals))\n",
    "\n",
    "    optimal_n_neighbours = n_list[np.argmax(mean_accs)]\n",
    "\n",
    "    optimal_model = KNeighborsClassifier(n_neighbors=optimal_n_neighbours)\n",
    "    optimal_model.fit(X_Train, y_Train)\n",
    "\n",
    "    y_Pred = optimal_model.predict(X_test)\n",
    "    acc = (y_Pred == y_test).sum() / (y_Pred.shape[0])\n",
    "    return optimal_n_neighbours, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bbfcc56c639c4b419d4f8b590955e9e",
     "grade": true,
     "grade_id": "cell-7870e89168cd3516",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your function found the optimal value of n_neighbors to be 6\n",
      "Your function calculated the accuracy of your final model as 0.976\n",
      "Passed tests\n"
     ]
    }
   ],
   "source": [
    "# Test cell\n",
    "neighbors_list = range(2,20,1)\n",
    "k=3\n",
    "opt_n, testacc = crossval_compare(X,y,neighbors_list,k,test_size=0.25,random_state=0)\n",
    "print('Your function found the optimal value of n_neighbors to be {}'.format(opt_n))\n",
    "print('Your function calculated the accuracy of your final model as {:.3f}'.format(testacc))\n",
    "\n",
    "assert (opt_n == 6)\n",
    "assert np.round(testacc,2) >= 0.98\n",
    "print('Passed tests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (12 pts)\n",
    "For each of the following tasks, please indicate what type of machine learning you would apply and support your answer with a justification as to why you have chosen that type.  Some of them could be accomplished in multiple ways – in this case please choose the single most likely approach.  If you select “supervised ML”, please also indicate if you would use a “regression” or “classification” model.  \n",
    "a)     Build a system capable of playing a complex card game  \n",
    "b)    Build a system that uses your video doorbell to recognize faces and alert you to who is at the door  \n",
    "c)     Build a system that can go through your emails and group them into groups of similar type to enable more efficient responding, without you having to explicitly specify what the groups should be  \n",
    "d)    Build a system that can predict your time in your next running race\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE:\n",
    "\n",
    "a) For a complex card game, you would need an ML algorithm like reinforcement learning, since it can learn from a rewards-punishment system and improve it's decision making over several iterations of playing the game.\n",
    "\n",
    "b) A facial recognition system would need to classify the different faces, so it would be a supervised learning algorithm, and it would be a classification model.\n",
    "\n",
    "c) Grouping without labeling would come under unsupervised learning, like a clustering algorithm of sorts. A k-means clustering could be useful.\n",
    "\n",
    "d) Prediction of time as a real number would be a supervised learning algorithm, of type regression, since a real number is to be predicted. A linear regression model could come in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "a)     Build a system capable of playing a complex card game  \n",
    "* 3pts\n",
    "* REINFORCEMENT LEARNING\n",
    "* For full credit must explain why, otherwise -1.5  \n",
    "\n",
    "b)    Build a system that uses your video doorbell to recognize faces and alert you to who is at the door  \n",
    "* 3pts\n",
    "* SUPERVISED LEARNING, CLASSIFICATION\n",
    "* For full credit must explain why, otherwise -1.5\n",
    "\n",
    "\n",
    "c)     Build a system that can go through your emails and group them into groups of similar type to enable more efficient responding, without you having to explicitly specify what the groups should be  \n",
    "* 3pts\n",
    "* UNSUPERVISED LEARNING\n",
    "* For full credit must explain why, otherwise -1.5\n",
    "\n",
    "\n",
    "d)    Build a system that can predict your time in your next running race\n",
    "* 3pts\n",
    "* SUPERVISED LEARNING, REGRESSION\n",
    "* For full credit must explain why, otherwise -1.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (13 pts)\n",
    "R-squared can be understood to be a standardized version of Mean Squared Error (MSE).  What is the mathematical relationship between R-squared and MSE for a regression model?  \n",
    " \n",
    "In your answer please use equations (for example showing R-squared in terms of MSE or MSE in terms of R-squared) and then also include text describing (in written words) what the equations you have included mean in plain language.  You can optionally include a graph/image if helpful, but do not need to do so for full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE:\n",
    "SSR is the sum of squares regression, which is the square of difference of predicted value and the mean of the dependent variable, summed over all data points:\n",
    "\n",
    "$$\n",
    "SSR = \\sum_{i=1}^n (\\widehat{y_i} - \\mu_y)^2\n",
    "$$\n",
    "\n",
    "SSE is the sum of squares error, which is the square of difference of predicted value and the true value of the dependent variable.\n",
    "$$\n",
    "SSE = \\sum_{i=1}^n (y_i - \\widehat{y_i})^2\n",
    "$$\n",
    "\n",
    "SST is the sum of squares total, which is the square of difference of true value of dependent variable and the mean.\n",
    "\n",
    "$$\n",
    "SST = \\sum_{i=1}^n (y_i - \\mu_y)^2\n",
    "$$\n",
    "\n",
    "If we write SST as follows:\n",
    "\n",
    "$$\n",
    "SST = \\sum_{i=1}^n (y_i - \\mu_y)^2 = \\sum_{i=1}^n (y_i - \\widehat{y_i} + \\widehat{y_i} - \\mu_y)^2 = \\sum_{i=1}^n (y_i - \\widehat{y_i})^2 + \\sum_{i=1}^n(\\widehat{y_i} - \\mu_y)^2 + 2.\\sum_{i=1}^n(y_i - \\widehat{y_i})((\\widehat{y_i} - \\mu_y))\n",
    "$$\n",
    "\n",
    "When we assume a linear regression model and optimize model parameters for the minimum square error, we get parameters such that the [last term in the last equation becomes 0](https://math.stackexchange.com/questions/709419/prove-sst-ssessr)\n",
    "\n",
    "So \n",
    "$$\n",
    "SST = SSE + SSR\n",
    "$$\n",
    "\n",
    "R^2 is defined as:\n",
    "$$\n",
    "R^2 = \\frac{SSR}{SST} = \\frac{SST - SSE}{SST} = 1 - \\frac{SSE}{SST} \n",
    "$$\n",
    "\n",
    "\n",
    "Dividing numerator and denominator by n in the last term, we get\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{MSE}{Var(y)}\n",
    "$$\n",
    "\n",
    "R^2 denotes how much of the variation in the dependent variable is predictable from the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "<!-- $$\n",
    "R^2=SSR/SST=1-SSE/SST=1-\\ (\\sum_(i=1)^n(y_i-y\\ \\widehat_i\\ )^2\\ )/(\\sum_(i=1)^n(y_i-\\mu_y\\ )^2\\ )=1-\\ \\ (1/n\\ \\sum_(i=1)^n\\of(y_i-y\\ \\widehat_i\\ )^2\\ )/(1/n\\ \\sum_(i=1)^n\\of(y_i-\\mu_y\\ )^2\\ )=1-MSE/(Var(y))\n",
    "$$ -->\n",
    "$$\n",
    "R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} = 1 - \\frac{\\sum_{i=1}^n (y_i - \\widehat{y_i})^2}{\\sum_{i=1}^n (y_i - \\mu_y)^2} = 1 - \\frac{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\widehat{y_i})^2}{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\mu_y)^2} = 1 - \\frac{MSE}{\\text{Var}(y)}\n",
    "$$\n",
    "\n",
    "* For full credit, student must also explain the above relationship in a couple of sentences.  \n",
    "* If student’s equations are incorrect, partial credit can be given depending on severity.  \n",
    "    * Very wrong -8\n",
    "    * Slightly wrong -4.  \n",
    "* If student put correct equations but did not include text describing them, -6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (10 pts)\n",
    "Suppose we are working on a binary classification problem where we know we have a high class imbalance in the target labels.  Answer the following questions:  \n",
    "1)    Should we use accuracy, recall, or F1 score as a metric for evaluation of our model?  Why?  \n",
    "2)    Instead suppose we would like to vary our model threshold and look at its robustness.  Would it be better to use the ROC curve (and AUROC) or the PR curve (and AP) to do so? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE:\n",
    "1. Since F1 score is the harmonic mean of precision and recall, it considers both metrics with equal weightage. Hence the F1 score should be used as a metric. \n",
    "\n",
    "2. For a high class imbalance, a PR curve would be better since it doesn't factor in true negatives. An ROC curve does factor in true negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1)    Should we use accuracy, recall, or F1 score as a metric for evaluation of our model?  Why?  \n",
    "* F1 because it factors in both precision and recall.  Models which only predict the dominant class can achieve high accuracy and recall with no skill at all.  \n",
    "    * 5 pts. -5 if wrong answer  \n",
    "    * -3 if right but no explanation is provided or incorrect explanation\n",
    "\n",
    "2)    Instead suppose we would like to vary our model threshold and look at its robustness.  Would it be better to use the ROC curve (and AUROC) or the PR curve (and AP) to do so? Why?  \n",
    "* PR curve because it does not include true negatives, which might be very high in our situation e.g. we have a lot of negatives and very few positives, so we do not want to give a lot of “credit” for predicting true negatives.  \n",
    "    * 5 pts. -5 if wrong answer  \n",
    "    * -3 if right but no explanation is provided or incorrect explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73a72a89c04eb6eeb337edd0d3a0cdb2",
     "grade": false,
     "grade_id": "cell-3a952c5752cb3e0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 5\n",
    "In this question we will be practicing different types of sampling.  We will be using a dataset containing health and demographic data about a number of patients.  Some of the patients have been diagnosed with heart disease (as indicated by the \"AHD\" column in the dataset) and other patients are normal.\n",
    "\n",
    "Note: for this question you may NOT use scikit-learn's `train_test_split()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5463a69edbef272c2efb1e5b803e695",
     "grade": false,
     "grade_id": "cell-0dac25cfbd4fab96",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPain</th>\n",
       "      <th>RestBP</th>\n",
       "      <th>Chol</th>\n",
       "      <th>Fbs</th>\n",
       "      <th>RestECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExAng</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Thal</th>\n",
       "      <th>AHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>typical</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fixed</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>nonanginal</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>nontypical</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Sex     ChestPain  RestBP  Chol  Fbs  RestECG  MaxHR  ExAng  Oldpeak  \\\n",
       "1   63    1       typical     145   233    1        2    150      0      2.3   \n",
       "2   67    1  asymptomatic     160   286    0        2    108      1      1.5   \n",
       "3   67    1  asymptomatic     120   229    0        2    129      1      2.6   \n",
       "4   37    1    nonanginal     130   250    0        0    187      0      3.5   \n",
       "5   41    0    nontypical     130   204    0        2    172      0      1.4   \n",
       "\n",
       "   Slope   Ca        Thal  AHD  \n",
       "1      3  0.0       fixed   No  \n",
       "2      2  3.0      normal  Yes  \n",
       "3      2  2.0  reversable  Yes  \n",
       "4      3  0.0      normal   No  \n",
       "5      1  0.0      normal   No  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This downloads the csv data files into the same directory where you have saved this notebook\n",
    "\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import os\n",
    "path = Path()\n",
    "\n",
    "# Dictionary of file names and download links\n",
    "files = {'Heart.csv':'https://storage.googleapis.com/aipi_datasets/Heart.csv'}\n",
    "\n",
    "# Download each file\n",
    "for key,value in files.items():\n",
    "    filename = path/key\n",
    "    url = value\n",
    "    # If the file does not already exist in the directory, download it\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url,filename)\n",
    "\n",
    "data = pd.read_csv('Heart.csv',index_col=0)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "858af0e326cb1f73eddaa0c512879c27",
     "grade": false,
     "grade_id": "cell-66921dff9f850f28",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q5.1 (10 pts)\n",
    "Complete the below function `random_sampling()` which takes as input the DataFrame of patient data and a percentage to use for the test set.  The function should separate the data into the inputs (X) and targets (y) and split the data into training and test sets.  It should then return the split data `X_train, y_train, X_test, y_test` as NumPy arrays.\n",
    "\n",
    "One consideration in splitting based on percentages is what to do when the target percentage for the test set does not equate to an integer.  In this situation please use the ceiling function to round up to the next integer.  For example, if we have 50 datapoints and I ask you to use 25% for the test set, your test set should include 13 points and the remaining 37 should be used for the training set.\n",
    "\n",
    "Note: for this question you may NOT use scikit-learn's `train_test_split()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fc85e184bb842c2fbd2e4ab81ae2dee",
     "grade": false,
     "grade_id": "cell-68c6758e46263022",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def random_sampling(data,pct):\n",
    "    '''\n",
    "    Randomly splits the data into training and test sets (without using train_test_split)\n",
    "\n",
    "    Inputs:\n",
    "        data(pd.DataFrame): DataFrame containing the inputs and labels\n",
    "        pct(float): percentage of data to use for the test set\n",
    "\n",
    "    Returns:\n",
    "        X_train(np.ndarray): training set inputs\n",
    "        y_train(np.ndarray): training set targets\n",
    "        X_test(np.ndarray): test set inputs\n",
    "        y_test(np.ndarray): test set targets\n",
    "    '''\n",
    "    X = data[[\"Age\", \"Sex\", \"ChestPain\", \"RestBP\", \"Chol\", \"Fbs\", \"RestECG\", \"MaxHR\", \"ExAng\", \"Oldpeak\", \"Slope\", \"Ca\", \"Thal\"]]\n",
    "    y = data[[\"AHD\"]]\n",
    "    \n",
    "    num_test_samples = int(np.ceil(y.size * pct))\n",
    "    idxs = np.arange(y.size)\n",
    "    test_idxs = np.random.choice(y.size, size = num_test_samples, replace = False)\n",
    "    train_idxs = list(set(idxs).difference(set(test_idxs)))\n",
    "    return X.iloc[train_idxs], y.iloc[train_idxs], X.iloc[test_idxs], y.iloc[test_idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07d0df7fd48b824203948548d8807521",
     "grade": true,
     "grade_id": "cell-1018bb290d84e4fc",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed tests\n"
     ]
    }
   ],
   "source": [
    "# Test cell\n",
    "def train_test_split():\n",
    "    return\n",
    "\n",
    "pct = 0.2\n",
    "X_train,y_train,X_test,y_test = random_sampling(data,pct)\n",
    "\n",
    "assert X_train.shape[0] == data.shape[0] - int(np.ceil(pct*data.shape[0]))\n",
    "assert X_train.shape[1] == data.shape[1] - 1\n",
    "assert y_train.shape[0] == X_train.shape[0]\n",
    "assert X_test.shape[0] == int(np.ceil(pct*data.shape[0]))\n",
    "assert y_test.shape[0] == X_test.shape[0]\n",
    "\n",
    "print('passed tests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d1c508c5764f37840e42f100913f04e",
     "grade": false,
     "grade_id": "cell-89b2a02307ec7310",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q5.2 (10 pts)\n",
    "Complete the below function `stratified_sampling()` which takes as input the DataFrame of patient data and a percentage to use for the test set.  The function should separate the data into the inputs (X) and targets (y) and split the data into training and test sets using stratified sampling based on the target classes to ensure that observations from each class are represented in both the training and test sets.  This means that it should use the specified percentage of points **from each class** in the test set.  It should then return the split data `X_train, y_train, X_test, y_test` as NumPy arrays.\n",
    "\n",
    "One consideration in splitting based on percentages is what to do when the target percentage for the test set does not equate to an integer.  In this situation please use the ceiling function to round up to the next integer.  For example, if we have 50 datapoints and I ask you to use 25% for the test set, your test set should include 13 points and the remaining 37 should be used for the training set.\n",
    "\n",
    "Note: for this question you may NOT use scikit-learn's `train_test_split()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8473406e6976892c8dc03466a96e20dc",
     "grade": false,
     "grade_id": "cell-82c7229534ebbe0a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def stratified_sampling(data,pct):\n",
    "    '''\n",
    "    Splits the data into training and test sets using stratified sampling based on the target classes (without using train_test_split)\n",
    "\n",
    "    Inputs:\n",
    "        data(pd.DataFrame): DataFrame containing the inputs and labels\n",
    "        pct(float): percentage of data to use for the test set\n",
    "\n",
    "    Returns:\n",
    "        X_train(np.ndarray): training set inputs\n",
    "        y_train(np.ndarray): training set targets\n",
    "        X_test(np.ndarray): test set inputs\n",
    "        y_test(np.ndarray): test set targets\n",
    "    '''\n",
    "    X = data[[\"Age\", \"Sex\", \"ChestPain\", \"RestBP\", \"Chol\", \"Fbs\", \"RestECG\", \"MaxHR\", \"ExAng\", \"Oldpeak\", \"Slope\", \"Ca\", \"Thal\"]]\n",
    "    y = np.array(data[[\"AHD\"]])\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = pct)\n",
    "\n",
    "    num_test_samples = int(np.ceil(y.size * pct))\n",
    "    idxs = np.arange(y.size)\n",
    "    test_idxs = np.random.choice(y.size, size = num_test_samples, replace = False)\n",
    "    train_idxs = list(set(idxs).difference(set(test_idxs)))\n",
    "    return X.iloc[train_idxs], y[train_idxs], X.iloc[test_idxs], y[test_idxs]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcbae40f7c98c908881c75d29ab57e1f",
     "grade": true,
     "grade_id": "cell-de63be336803cc5d",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed tests\n"
     ]
    }
   ],
   "source": [
    "# Test cell\n",
    "\n",
    "def train_test_split():\n",
    "    return\n",
    "\n",
    "pct = 0.2\n",
    "X_train,y_train,X_test,y_test = stratified_sampling(data,pct)\n",
    "\n",
    "assert X_train.shape[0] == data.shape[0] - int(np.ceil(pct*data.shape[0]))\n",
    "assert X_train.shape[1] == data.shape[1] - 1\n",
    "assert y_train.shape[0] == X_train.shape[0]\n",
    "assert X_test.shape[0] == int(np.ceil(pct*data.shape[0]))\n",
    "assert y_test.shape[0] == X_test.shape[0]\n",
    "\n",
    "assert np.round(len(y_train[y_train=='Yes']) / data.loc[data['AHD']=='Yes',:].shape[0],1) == 1 - pct\n",
    "assert np.round(len(y_test[y_test=='Yes']) / data.loc[data['AHD']=='Yes',:].shape[0],1) == pct\n",
    "\n",
    "print('passed tests')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
